#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç°¡åŒ–ç‰ˆ NER å¯¦ä½œ
åªä½¿ç”¨ OpenAI API é€²è¡Œå¯¦é«”è­˜åˆ¥
"""

import pandas as pd
from openai import OpenAI
import json
import time
import os
from typing import List, Dict, Any

# è¼‰å…¥ .env æª”æ¡ˆ
def load_env_file():
    """è¼‰å…¥ .env æª”æ¡ˆ"""
    env_file = '.env'
    if os.path.exists(env_file):
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()

# åœ¨ç¨‹å¼é–‹å§‹æ™‚è¼‰å…¥ .env æª”æ¡ˆ
load_env_file()

class SimpleNER:
    """ç°¡åŒ–ç‰ˆå‘½åå¯¦é«”è­˜åˆ¥"""
    
    def __init__(self):
        # å¾ç’°å¢ƒè®Šæ•¸è®€å– API Key
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("è«‹è¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        
        self.client = OpenAI(api_key=api_key)
    
    def extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ OpenAI API é€²è¡Œå¯¦é«”è­˜åˆ¥"""
        try:
            prompt = f"""
è«‹å¾ä»¥ä¸‹æ–‡æœ¬ä¸­è­˜åˆ¥ä¸¦æå–å‘½åå¯¦é«”ï¼ŒåŒ…æ‹¬ï¼š
- äººåï¼ˆPERSONï¼‰ï¼šäººç‰©åç¨±
- åœ°åï¼ˆLOCATIONï¼‰ï¼šåœ°é»ã€åŸå¸‚ã€åœ‹å®¶
- çµ„ç¹”ï¼ˆORGANIZATIONï¼‰ï¼šå…¬å¸ã€æ©Ÿæ§‹
- æ—¥æœŸï¼ˆDATEï¼‰ï¼šæ™‚é–“è¡¨é”å¼
- é‡‘é¡ï¼ˆMONEYï¼‰ï¼šè²¨å¹£é‡‘é¡

æ–‡æœ¬ï¼š{text}

è«‹ä»¥ JSON æ ¼å¼è¿”å›çµæœï¼š
{{
    "entities": [
        {{"text": "å¯¦é«”æ–‡æœ¬", "label": "å¯¦é«”é¡å‹", "confidence": 0.95}}
    ]
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
"""
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1
            )
            
            content = response.choices[0].message.content.strip()
            
            # å˜—è©¦æ¸…ç†å›æ‡‰å…§å®¹ï¼Œç§»é™¤å¯èƒ½çš„é JSON éƒ¨åˆ†
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            content = content.strip()
            
            result = json.loads(content)
            return result.get('entities', [])
            
        except json.JSONDecodeError as e:
            print(f"JSON è§£æéŒ¯èª¤: {e}")
            print(f"å›æ‡‰å…§å®¹: {content[:200]}...")
            return []
        except Exception as e:
            print(f"API éŒ¯èª¤: {e}")
            return []

def demonstrate_ner_applications():
    """å±•ç¤º NER åœ¨ä¸åŒæ‡‰ç”¨å ´æ™¯ä¸­çš„ä½¿ç”¨"""
    
    ner = SimpleNER()
    
    # æ‡‰ç”¨å ´æ™¯ 1: æ–°èæå–
    print("ğŸ“° æ‡‰ç”¨å ´æ™¯ 1: æ–°èæå–")
    print("=" * 50)
    news_text = "å°ç£åŠå°é«”é¾é ­å°ç©é›»ï¼ˆTSMCï¼‰ä»Šæ—¥å®£å¸ƒï¼Œå°‡åœ¨å°å—ç§‘å­¸åœ’å€æŠ•è³‡æ–°å°å¹£1000å„„å…ƒï¼Œèˆˆå»º3å¥ˆç±³è£½ç¨‹æ™¶åœ“å» ã€‚è‘£äº‹é•·åŠ‰å¾·éŸ³è¡¨ç¤ºï¼Œæ­¤æŠ•è³‡å°‡å‰µé€ 5000å€‹å°±æ¥­æ©Ÿæœƒï¼Œé è¨ˆ2025å¹´æŠ•ç”¢ã€‚"
    
    print(f"æ–°èå…§å®¹ï¼š{news_text}")
    print("\næå–çš„å¯¦é«”ï¼š")
    news_entities = ner.extract_entities(news_text)
    display_entities(news_entities)
    
    time.sleep(2)  # é¿å… API é™åˆ¶
    
    # æ‡‰ç”¨å ´æ™¯ 2: éƒµä»¶åˆ†é¡
    print("\nğŸ“§ æ‡‰ç”¨å ´æ™¯ 2: éƒµä»¶åˆ†é¡")
    print("=" * 50)
    email_text = "è¦ªæ„›çš„å¼µç¶“ç†ï¼Œæˆ‘æ˜¯ABCå…¬å¸çš„æ¥­å‹™ä»£è¡¨ææ˜ï¼Œæƒ³èˆ‡æ‚¨è¨è«–ä¸‹é€±ä¸‰ï¼ˆ2024å¹´1æœˆ15æ—¥ï¼‰çš„æœƒè­°å®‰æ’ã€‚æˆ‘å€‘å…¬å¸ä½æ–¼å°åŒ—å¸‚ä¿¡ç¾©å€ï¼Œå¸Œæœ›èƒ½åœ¨ä¸‹åˆ2é»èˆ‡æ‚¨æœƒé¢ã€‚é ç®—ç´„ç‚ºæ–°å°å¹£50è¬å…ƒã€‚"
    
    print(f"éƒµä»¶å…§å®¹ï¼š{email_text}")
    print("\næå–çš„å¯¦é«”ï¼š")
    email_entities = ner.extract_entities(email_text)
    display_entities(email_entities)
    
    time.sleep(2)
    
    # æ‡‰ç”¨å ´æ™¯ 3: ç°¡æ­·è§£æ
    print("\nğŸ’¼ æ‡‰ç”¨å ´æ™¯ 3: ç°¡æ­·è§£æ")
    print("=" * 50)
    resume_text = "å§“åï¼šé™³å°è¯ï¼Œå­¸æ­·ï¼šå°ç£å¤§å­¸è³‡è¨Šå·¥ç¨‹å­¸ç³»ï¼ˆ2018-2022ï¼‰ï¼Œå·¥ä½œç¶“é©—ï¼š2022å¹´6æœˆè‡³ä»Šï¼šGoogleå°ç£åˆ†å…¬å¸ï¼Œè»Ÿé«”å·¥ç¨‹å¸«ï¼ŒæœŸæœ›è–ªè³‡ï¼šæœˆè–ªæ–°å°å¹£8è¬å…ƒ"
    
    print(f"ç°¡æ­·å…§å®¹ï¼š{resume_text}")
    print("\næå–çš„å¯¦é«”ï¼š")
    resume_entities = ner.extract_entities(resume_text)
    display_entities(resume_entities)
    
    time.sleep(2)
    
    # æ‡‰ç”¨å ´æ™¯ 4: åˆåŒåˆ†æ
    print("\nğŸ“ æ‡‰ç”¨å ´æ™¯ 4: åˆåŒåˆ†æ")
    print("=" * 50)
    contract_text = "ç”²æ–¹ï¼šå°ç£ç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸ï¼Œä¹™æ–¹ï¼šå‰µæ–°è»Ÿé«”æœ‰é™å…¬å¸ï¼Œç°½ç´„æ—¥æœŸï¼š2024å¹´1æœˆ10æ—¥ï¼Œåˆç´„é‡‘é¡ï¼šæ–°å°å¹£200è¬å…ƒï¼Œå±¥ç´„æœŸé™ï¼š2024å¹´12æœˆ31æ—¥"
    
    print(f"åˆåŒå…§å®¹ï¼š{contract_text}")
    print("\næå–çš„å¯¦é«”ï¼š")
    contract_entities = ner.extract_entities(contract_text)
    display_entities(contract_entities)
    
    time.sleep(2)
    
    # æ‡‰ç”¨å ´æ™¯ 5: é†«ç™‚è¨˜éŒ„
    print("\nğŸ¥ æ‡‰ç”¨å ´æ™¯ 5: é†«ç™‚è¨˜éŒ„")
    print("=" * 50)
    medical_text = "æ‚£è€…ï¼šç‹å°æ˜ï¼Œç”·ï¼Œ35æ­²ï¼Œè¨ºæ–·ï¼šé«˜è¡€å£“ï¼Œè™•æ–¹è—¥ç‰©ï¼šAmlodipine 5mgï¼Œæ¯æ—¥ä¸€æ¬¡ï¼Œä¸‹æ¬¡å›è¨ºï¼š2024å¹´2æœˆ15æ—¥ï¼Œè¡€å£“ï¼šæ”¶ç¸®å£“140mmHgï¼Œèˆ’å¼µå£“90mmHg"
    
    print(f"é†«ç™‚è¨˜éŒ„ï¼š{medical_text}")
    print("\næå–çš„å¯¦é«”ï¼š")
    medical_entities = ner.extract_entities(medical_text)
    display_entities(medical_entities)
    
    return {
        'news': news_entities,
        'email': email_entities,
        'resume': resume_entities,
        'contract': contract_entities,
        'medical': medical_entities
    }

def display_entities(entities: List[Dict[str, Any]]):
    """é¡¯ç¤ºå¯¦é«”çµæœ"""
    if not entities:
        print("  æœªæ‰¾åˆ°ä»»ä½•å¯¦é«”")
        return
    
    # æŒ‰é¡å‹åˆ†çµ„
    entity_groups = {}
    for entity in entities:
        label = entity.get('label', 'UNKNOWN')
        if label not in entity_groups:
            entity_groups[label] = []
        entity_groups[label].append(entity)
    
    # é¡¯ç¤ºçµæœ
    for label, group in entity_groups.items():
        print(f"  {label}:")
        for entity in group:
            confidence = entity.get('confidence', 0)
            print(f"    - {entity['text']} (å¯ä¿¡åº¦: {confidence:.2f})")

def batch_process_texts(texts: List[str], batch_size: int = 3) -> pd.DataFrame:
    """æ‰¹é‡è™•ç†æ–‡æœ¬ä¸¦é€²è¡Œå¯¦é«”è­˜åˆ¥"""
    ner = SimpleNER()
    results = []
    
    print(f"\nğŸ”„ æ‰¹é‡è™•ç† {len(texts)} å€‹æ–‡æœ¬...")
    print("=" * 50)
    
    for i, text in enumerate(texts):
        print(f"è™•ç†æ–‡æœ¬ {i+1}/{len(texts)}")
        
        try:
            entities = ner.extract_entities(text)
            
            result = {
                'text_id': i,
                'text': text,
                'entities': json.dumps(entities, ensure_ascii=False),
                'entity_count': len(entities),
                'processed': True
            }
            
            results.append(result)
            
            # é¡¯ç¤ºæå–çš„å¯¦é«”
            if entities:
                print(f"  æ‰¾åˆ° {len(entities)} å€‹å¯¦é«”:")
                for entity in entities:
                    print(f"    - {entity['text']} ({entity['label']})")
            else:
                print("  æœªæ‰¾åˆ°å¯¦é«”")
            
        except Exception as e:
            print(f"  è™•ç†éŒ¯èª¤: {e}")
            results.append({
                'text_id': i,
                'text': text,
                'entities': json.dumps([], ensure_ascii=False),
                'entity_count': 0,
                'processed': False,
                'error': str(e)
            })
        
        # æ‰¹æ¬¡é–“æš«åœ
        if (i + 1) % batch_size == 0:
            print("  æš«åœ 2 ç§’...")
            time.sleep(2)
    
    return pd.DataFrame(results)

def analyze_results(df: pd.DataFrame):
    """åˆ†æè™•ç†çµæœ"""
    print("\nğŸ“Š çµæœåˆ†æ")
    print("=" * 50)
    
    # åŸºæœ¬çµ±è¨ˆ
    total_texts = len(df)
    processed_texts = df['processed'].sum()
    total_entities = df['entity_count'].sum()
    
    print(f"ç¸½æ–‡æœ¬æ•¸: {total_texts}")
    print(f"æˆåŠŸè™•ç†: {processed_texts}")
    print(f"ç¸½å¯¦é«”æ•¸: {total_entities}")
    print(f"å¹³å‡æ¯æ–‡æœ¬å¯¦é«”æ•¸: {total_entities/processed_texts:.2f}" if processed_texts > 0 else "å¹³å‡æ¯æ–‡æœ¬å¯¦é«”æ•¸: 0")
    
    # å¯¦é«”é¡å‹çµ±è¨ˆ
    all_entities = []
    for entities_json in df['entities']:
        try:
            entities = json.loads(entities_json)
            all_entities.extend(entities)
        except:
            continue
    
    if all_entities:
        entity_types = [entity.get('label', 'UNKNOWN') for entity in all_entities]
        type_counts = {}
        for entity_type in entity_types:
            type_counts[entity_type] = type_counts.get(entity_type, 0) + 1
        
        print(f"\nå¯¦é«”é¡å‹åˆ†å¸ƒ:")
        for entity_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  {entity_type}: {count}")
        
        # å¯ä¿¡åº¦çµ±è¨ˆ
        confidences = [entity.get('confidence', 0) for entity in all_entities]
        if confidences:
            print(f"\nå¯ä¿¡åº¦çµ±è¨ˆ:")
            print(f"  å¹³å‡å¯ä¿¡åº¦: {sum(confidences)/len(confidences):.2f}")
            print(f"  æœ€é«˜å¯ä¿¡åº¦: {max(confidences):.2f}")
            print(f"  æœ€ä½å¯ä¿¡åº¦: {min(confidences):.2f}")

def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸš€ å‘½åå¯¦é«”è­˜åˆ¥ï¼ˆNERï¼‰å¯¦ä½œç¯„ä¾‹")
    print("=" * 60)
    
    # å±•ç¤ºä¸åŒæ‡‰ç”¨å ´æ™¯
    all_results = demonstrate_ner_applications()
    
    # æ‰¹é‡è™•ç†ç¯„ä¾‹
    print("\n" + "=" * 60)
    print("ğŸ“‹ æ‰¹é‡è™•ç†ç¯„ä¾‹")
    print("=" * 60)
    
    # æº–å‚™æ¸¬è©¦æ•¸æ“š
    test_texts = [
        "å°ç£åŠå°é«”é¾é ­å°ç©é›»å°‡åœ¨å°å—æŠ•è³‡1000å„„å…ƒ",
        "å¼µç¶“ç†ï¼Œæˆ‘å€‘ä¸‹é€±ä¸‰åœ¨å°åŒ—å¸‚ä¿¡ç¾©å€æœƒé¢",
        "é™³å°è¯ç•¢æ¥­æ–¼å°ç£å¤§å­¸ï¼Œç¾ä»»Googleå·¥ç¨‹å¸«",
        "ç”²æ–¹ï¼šå°ç£ç§‘æŠ€å…¬å¸ï¼Œåˆç´„é‡‘é¡200è¬å…ƒ",
        "æ‚£è€…ç‹å°æ˜ï¼Œè¨ºæ–·é«˜è¡€å£“ï¼Œè™•æ–¹Amlodipine"
    ]
    
    # æ‰¹é‡è™•ç†
    results_df = batch_process_texts(test_texts)
    
    # åˆ†æçµæœ
    analyze_results(results_df)
    
    # ä¿å­˜çµæœ
    results_df.to_csv('ner_labeled_data.csv', index=False, encoding='utf-8')
    print(f"\nâœ… çµæœå·²ä¿å­˜è‡³ 'ner_labeled_data.csv'")
    
    print("\nğŸ‰ NER å¯¦ä½œç¯„ä¾‹å®Œæˆï¼")

if __name__ == "__main__":
    main()
